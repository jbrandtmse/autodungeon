"""Integration tests for LLM functionality with real API calls.

These tests exercise the actual Gemini API to verify the system works
end-to-end with real LLMs. They are marked with @pytest.mark.integration
and can be run separately from unit tests.

To run integration tests:
    pytest tests/test_integration_llm.py -v --tb=short

Requirements:
    - tests/integration_config.py must exist with GOOGLE_API_KEY
    - Internet connection to reach Gemini API
    - Valid API key with sufficient quota

Note: These tests make real API calls and may incur costs.
"""

from __future__ import annotations

import os
import warnings
from typing import TYPE_CHECKING
from unittest.mock import MagicMock, patch

import pytest

if TYPE_CHECKING:
    from models import CharacterConfig, DMConfig, GameState

# =============================================================================
# Configuration Loading with Warnings
# =============================================================================

# Try to load integration config
_INTEGRATION_CONFIG_AVAILABLE = False
_GOOGLE_API_KEY: str | None = None
_DM_MODEL = "gemini-1.5-pro"  # fallback
_PC_MODEL = "gemini-1.5-flash"  # fallback

try:
    from tests.integration_config import DM_MODEL, GOOGLE_API_KEY, PC_MODEL

    _GOOGLE_API_KEY = GOOGLE_API_KEY
    _DM_MODEL = DM_MODEL
    _PC_MODEL = PC_MODEL
    _INTEGRATION_CONFIG_AVAILABLE = True
except ImportError:
    warnings.warn(
        "\n"
        "=" * 70 + "\n"
        "INTEGRATION TEST CONFIG NOT FOUND\n"
        "=" * 70 + "\n"
        "tests/integration_config.py is missing.\n"
        "\n"
        "To enable integration tests:\n"
        "1. Create tests/integration_config.py with:\n"
        '   GOOGLE_API_KEY = "your-api-key-here"\n'
        '   DM_MODEL = "gemini-3-pro-preview"\n'
        '   PC_MODEL = "gemini-3-flash-preview"\n'
        "\n"
        "2. This file is gitignored, so your API key stays secure.\n"
        "=" * 70,
        stacklevel=1,
    )

# Skip all tests if config not available
pytestmark = pytest.mark.skipif(
    not _INTEGRATION_CONFIG_AVAILABLE,
    reason="Integration config (tests/integration_config.py) not found. "
    "Create this file with GOOGLE_API_KEY to run integration tests.",
)


# =============================================================================
# Module-level Setup (avoid repeated imports)
# =============================================================================

# Set environment before any imports that might use it
if _GOOGLE_API_KEY:
    os.environ["GOOGLE_API_KEY"] = _GOOGLE_API_KEY


# =============================================================================
# Fixtures
# =============================================================================


@pytest.fixture
def mock_streamlit() -> MagicMock:
    """Mock Streamlit session_state to avoid import errors in agent context building."""
    mock_st = MagicMock()
    mock_st.session_state = {}
    return mock_st


@pytest.fixture
def dm_config() -> DMConfig:
    """Create a DMConfig using integration test model."""
    from models import DMConfig

    return DMConfig(
        name="Dungeon Master",
        provider="gemini",
        model=_DM_MODEL,
        token_limit=8000,
    )


@pytest.fixture
def pc_configs() -> dict[str, CharacterConfig]:
    """Create CharacterConfigs for test characters using integration test model."""
    from models import CharacterConfig

    return {
        "fighter": CharacterConfig(
            name="Thorin",
            character_class="Fighter",
            personality="Bold and protective, always ready for battle",
            color="#8B4513",
            provider="gemini",
            model=_PC_MODEL,
            token_limit=4000,
        ),
        "rogue": CharacterConfig(
            name="Shadowmere",
            character_class="Rogue",
            personality="Cunning and stealthy, prefers to strike from shadows",
            color="#6B8E6B",
            provider="gemini",
            model=_PC_MODEL,
            token_limit=4000,
        ),
    }


@pytest.fixture
def game_state(
    dm_config: DMConfig, pc_configs: dict[str, CharacterConfig]
) -> GameState:
    """Create a minimal GameState for integration testing."""
    from models import (
        AgentMemory,
        CharacterFacts,
        GameConfig,
        GameState,
        NarrativeElementStore,
    )

    # Build turn queue
    turn_queue = ["dm"] + list(pc_configs.keys())

    # Initialize agent memories with CharacterFacts
    agent_memories: dict[str, AgentMemory] = {
        "dm": AgentMemory(token_limit=dm_config.token_limit)
    }
    for char_name, char_config in pc_configs.items():
        facts = CharacterFacts(
            name=char_config.name,
            character_class=char_config.character_class,
        )
        agent_memories[char_name] = AgentMemory(
            token_limit=char_config.token_limit,
            character_facts=facts,
        )

    return GameState(
        ground_truth_log=[],
        turn_queue=turn_queue,
        current_turn="dm",
        agent_memories=agent_memories,
        game_config=GameConfig(
            summarizer_provider="gemini",
            summarizer_model=_PC_MODEL,
        ),
        dm_config=dm_config,
        characters=pc_configs,
        whisper_queue=[],
        human_active=False,
        controlled_character=None,
        session_number=1,
        session_id="integration_test",
        summarization_in_progress=False,
        narrative_elements={},
        callback_database=NarrativeElementStore(),
    )


@pytest.fixture
def game_state_with_context(
    game_state: GameState,
) -> GameState:
    """Create a GameState with existing context in short_term_buffer."""
    from models import AgentMemory

    # Add some context to the buffers
    game_state["ground_truth_log"] = [
        "[dm] You stand at the entrance of a dark cave. A cold wind blows from within.",
        "[fighter] *Thorin draws his sword and steps forward.* 'I'll take point.'",
        "[rogue] *Shadowmere melts into the shadows nearby.* 'I'll watch our flanks.'",
    ]

    # Update agent memories with the context
    dm_memory = game_state["agent_memories"]["dm"]
    game_state["agent_memories"]["dm"] = AgentMemory(
        long_term_summary="The party has been exploring the wilderness for two days.",
        short_term_buffer=[
            "[dm] You stand at the entrance of a dark cave.",
            "[fighter] Thorin steps forward cautiously.",
        ],
        token_limit=dm_memory.token_limit,
    )

    fighter_memory = game_state["agent_memories"]["fighter"]
    game_state["agent_memories"]["fighter"] = AgentMemory(
        short_term_buffer=[
            "[dm] You stand at the entrance of a dark cave.",
            "[fighter] I draw my sword and take point.",
        ],
        token_limit=fighter_memory.token_limit,
        character_facts=fighter_memory.character_facts,
    )

    return game_state


# =============================================================================
# LLM Factory Tests
# =============================================================================


@pytest.mark.integration
class TestLLMFactory:
    """Test the LLM factory with real API connections."""

    def test_get_llm_gemini_creates_client(self) -> None:
        """Test that get_llm creates a working Gemini client."""
        from langchain_google_genai import ChatGoogleGenerativeAI

        from agents import get_llm

        llm = get_llm("gemini", _DM_MODEL)

        assert llm is not None
        assert isinstance(llm, ChatGoogleGenerativeAI)

    def test_get_llm_gemini_with_different_models(self) -> None:
        """Test that get_llm works with both DM and PC models."""
        from agents import get_llm

        dm_llm = get_llm("gemini", _DM_MODEL)
        pc_llm = get_llm("gemini", _PC_MODEL)

        assert dm_llm is not None
        assert pc_llm is not None

    def test_get_llm_without_api_key_raises(self) -> None:
        """Test that missing API key raises LLMConfigurationError."""
        from unittest.mock import MagicMock, patch

        from agents import LLMConfigurationError, get_llm

        # Temporarily remove API key from environment
        original = os.environ.pop("GOOGLE_API_KEY", None)

        # Reset config singleton to pick up env change
        import config as config_module

        old_config = config_module._config
        config_module._config = None

        try:
            # Mock both load_user_settings and get_config to have no API key
            mock_config = MagicMock()
            mock_config.google_api_key = None
            mock_config.anthropic_api_key = None
            mock_config.ollama_base_url = "http://localhost:11434"

            with (
                patch("agents.load_user_settings", return_value={}),
                patch("agents.get_config", return_value=mock_config),
                pytest.raises(LLMConfigurationError) as exc_info,
            ):
                get_llm("gemini", _DM_MODEL)

            assert exc_info.value.provider == "gemini"
            assert "GOOGLE_API_KEY" in str(exc_info.value)
        finally:
            # Restore API key and config
            if original:
                os.environ["GOOGLE_API_KEY"] = original
            config_module._config = old_config


# =============================================================================
# Agent Creation Tests
# =============================================================================


@pytest.mark.integration
class TestAgentCreation:
    """Test agent creation with real LLM clients."""

    def test_create_dm_agent_with_tools(self, dm_config: DMConfig) -> None:
        """Test that create_dm_agent binds dice rolling tools."""
        from agents import create_dm_agent

        agent = create_dm_agent(dm_config)

        assert agent is not None
        assert hasattr(agent, "invoke")

    def test_create_pc_agent_with_tools(
        self, pc_configs: dict[str, CharacterConfig]
    ) -> None:
        """Test that create_pc_agent binds dice rolling tools."""
        from agents import create_pc_agent

        for char_config in pc_configs.values():
            agent = create_pc_agent(char_config)
            assert agent is not None
            assert hasattr(agent, "invoke")

    def test_build_pc_system_prompt(
        self, pc_configs: dict[str, CharacterConfig]
    ) -> None:
        """Test that system prompts are built correctly for each class."""
        from agents import CLASS_GUIDANCE, build_pc_system_prompt

        for char_config in pc_configs.values():
            prompt = build_pc_system_prompt(char_config)

            assert char_config.name in prompt
            assert char_config.character_class in prompt
            assert char_config.personality in prompt

            if char_config.character_class in CLASS_GUIDANCE:
                assert "As a" in prompt


# =============================================================================
# Context Building Tests
# =============================================================================


@pytest.mark.integration
class TestContextBuilding:
    """Test context building functions."""

    def test_build_dm_context_includes_all_memories(
        self, game_state_with_context: GameState
    ) -> None:
        """Test that DM context includes all agent memories (asymmetric access)."""
        from agents import _build_dm_context

        context = _build_dm_context(game_state_with_context)

        # DM should see story summary or recent events
        assert "Story So Far" in context or "Recent Events" in context

    def test_build_pc_context_only_includes_own_memory(
        self, game_state_with_context: GameState
    ) -> None:
        """Test that PC context only includes the PC's own memory (isolation)."""
        from agents import _build_pc_context

        fighter_context = _build_pc_context(game_state_with_context, "fighter")

        # Fighter should see their own memory
        assert (
            "Character Identity" in fighter_context
            or "Recent Events" in fighter_context
        )

    def test_format_character_facts(self) -> None:
        """Test that CharacterFacts are formatted correctly."""
        from agents import format_character_facts
        from models import CharacterFacts

        facts = CharacterFacts(
            name="Thorin",
            character_class="Fighter",
            key_traits=["brave", "loyal"],
            relationships={"Shadowmere": "trusted ally"},
            notable_events=["Defended the village from goblins"],
        )

        formatted = format_character_facts(facts)

        assert "Thorin" in formatted
        assert "Fighter" in formatted
        assert "brave" in formatted
        assert "Shadowmere" in formatted


# =============================================================================
# Single Turn Tests (Real LLM Calls)
# =============================================================================


@pytest.mark.integration
class TestSingleTurn:
    """Test single turn execution with real LLM calls."""

    def test_dm_turn_generates_response(
        self, game_state: GameState, mock_streamlit: MagicMock
    ) -> None:
        """Test that DM turn generates a narrative response."""
        from agents import LLMError, dm_turn

        with patch.dict("sys.modules", {"streamlit": mock_streamlit}):
            try:
                result = dm_turn(game_state)
            except LLMError as e:
                if e.error_type == "rate_limit":
                    pytest.skip(f"Rate limit hit: {e}")
                raise

        # Verify DM generated a response
        assert len(result["ground_truth_log"]) > 0
        assert result["ground_truth_log"][0].startswith("[DM]:")

        # Verify DM's memory was updated
        dm_memory = result["agent_memories"]["dm"]
        assert len(dm_memory.short_term_buffer) > 0

        # Verify current_turn is set correctly
        assert result["current_turn"] == "dm"

    def test_pc_turn_generates_response(
        self, game_state_with_context: GameState, mock_streamlit: MagicMock
    ) -> None:
        """Test that PC turn generates an in-character response."""
        from agents import LLMError, pc_turn

        with patch.dict("sys.modules", {"streamlit": mock_streamlit}):
            try:
                result = pc_turn(game_state_with_context, "fighter")
            except LLMError as e:
                if e.error_type == "rate_limit":
                    pytest.skip(f"Rate limit hit: {e}")
                raise

        # Verify PC generated a response
        new_entries = result["ground_truth_log"][
            len(game_state_with_context["ground_truth_log"]) :
        ]
        assert len(new_entries) > 0

        # The response should be attributed to the character
        last_entry = new_entries[-1]
        assert "[Thorin]:" in last_entry

        # Verify PC's memory was updated
        fighter_memory = result["agent_memories"]["fighter"]
        assert len(fighter_memory.short_term_buffer) > len(
            game_state_with_context["agent_memories"]["fighter"].short_term_buffer
        )

    def test_dm_turn_with_empty_context(
        self,
        mock_streamlit: MagicMock,
        dm_config: DMConfig,
        pc_configs: dict[str, CharacterConfig],
    ) -> None:
        """Test DM turn works with no prior context."""
        from agents import LLMError, dm_turn
        from models import AgentMemory, GameConfig, GameState, NarrativeElementStore

        # Create minimal state with no context
        empty_state = GameState(
            ground_truth_log=[],
            turn_queue=["dm", "fighter"],
            current_turn="dm",
            agent_memories={"dm": AgentMemory(token_limit=8000)},
            game_config=GameConfig(),
            dm_config=dm_config,
            characters=pc_configs,
            whisper_queue=[],
            human_active=False,
            controlled_character=None,
            session_number=1,
            session_id="test",
            summarization_in_progress=False,
            narrative_elements={},
            callback_database=NarrativeElementStore(),
        )

        with patch.dict("sys.modules", {"streamlit": mock_streamlit}):
            try:
                result = dm_turn(empty_state)
            except LLMError as e:
                if e.error_type == "rate_limit":
                    pytest.skip(f"Rate limit hit: {e}")
                raise

        # DM should still generate something
        assert len(result["ground_truth_log"]) > 0


# =============================================================================
# Full Round Tests (Real LLM Calls)
# =============================================================================


@pytest.mark.integration
class TestFullRound:
    """Test full game rounds with real LLM calls."""

    def test_run_single_round_executes_all_agents(
        self, game_state: GameState, mock_streamlit: MagicMock
    ) -> None:
        """Test that run_single_round executes DM and all PCs."""
        from graph import run_single_round

        with (
            patch.dict("sys.modules", {"streamlit": mock_streamlit}),
            patch("persistence.save_checkpoint"),
            patch("persistence.get_latest_checkpoint", return_value=None),
            patch("graph._append_transcript_for_new_entries"),
        ):
            result = run_single_round(game_state)

        # Check for rate limit error and skip if present
        if "error" in result:
            error = result["error"]
            if hasattr(error, "error_type") and error.error_type == "rate_limit":
                pytest.skip(f"Rate limit hit: {error}")
            pytest.fail(f"Unexpected error during round: {error}")

        # Verify multiple entries were added (DM + PCs)
        log = result["ground_truth_log"]
        assert len(log) >= 2, f"Expected at least 2 entries, got {len(log)}"

        # Verify DM acted
        dm_entries = [e for e in log if e.startswith("[DM]:")]
        assert len(dm_entries) >= 1, "DM should have acted"

        # Verify at least one PC acted
        pc_entries = [e for e in log if not e.startswith("[DM]:")]
        assert len(pc_entries) >= 1, "At least one PC should have acted"

    def test_round_preserves_state_structure(
        self, game_state: GameState, mock_streamlit: MagicMock
    ) -> None:
        """Test that game state structure is preserved after a round."""
        from graph import run_single_round

        with (
            patch.dict("sys.modules", {"streamlit": mock_streamlit}),
            patch("persistence.save_checkpoint"),
            patch("persistence.get_latest_checkpoint", return_value=None),
            patch("graph._append_transcript_for_new_entries"),
        ):
            result = run_single_round(game_state)

        if "error" in result:
            pytest.skip(f"LLM error during test: {result['error']}")

        # Verify all state fields are present
        required_fields = [
            "ground_truth_log",
            "turn_queue",
            "current_turn",
            "agent_memories",
            "game_config",
            "dm_config",
            "characters",
            "human_active",
            "session_id",
        ]
        for field in required_fields:
            assert field in result, f"Missing field: {field}"

        # Verify turn_queue is preserved
        assert result["turn_queue"] == game_state["turn_queue"]

        # Verify characters are preserved
        assert set(result["characters"].keys()) == set(game_state["characters"].keys())


# =============================================================================
# Memory Management Tests
# =============================================================================


@pytest.mark.integration
class TestMemoryManagement:
    """Test memory management with real LLM calls."""

    def test_memory_isolation_between_pcs(
        self, game_state_with_context: GameState
    ) -> None:
        """Test that PC agents maintain memory isolation."""
        from agents import _build_pc_context

        fighter_context = _build_pc_context(game_state_with_context, "fighter")
        rogue_context = _build_pc_context(game_state_with_context, "rogue")

        # Each PC should only see their own context
        # The contexts should be different (assuming different buffer contents)
        assert fighter_context != rogue_context or (
            fighter_context == "" and rogue_context == ""
        )

    def test_dm_has_access_to_all_memories(
        self, game_state_with_context: GameState
    ) -> None:
        """Test that DM can read all agent memories."""
        from agents import _build_dm_context

        dm_context = _build_dm_context(game_state_with_context)

        # DM context should be more comprehensive than any single PC
        assert len(dm_context) > 0


# =============================================================================
# Workflow Tests
# =============================================================================


@pytest.mark.integration
class TestWorkflow:
    """Test LangGraph workflow creation and execution."""

    def test_create_game_workflow(self, game_state: GameState) -> None:
        """Test that game workflow can be created."""
        from graph import create_game_workflow

        workflow = create_game_workflow(game_state["turn_queue"])

        assert workflow is not None
        assert hasattr(workflow, "invoke")

    def test_workflow_with_custom_turn_queue(self) -> None:
        """Test workflow creation with custom turn queue."""
        from graph import create_game_workflow

        custom_queue = ["dm", "fighter"]
        workflow = create_game_workflow(custom_queue)

        assert workflow is not None

    def test_context_manager_node(
        self, game_state: GameState, mock_streamlit: MagicMock
    ) -> None:
        """Test the context_manager node processes without errors."""
        from graph import context_manager

        with patch.dict("sys.modules", {"streamlit": mock_streamlit}):
            result = context_manager(game_state)

        # Should return a valid state
        assert "summarization_in_progress" in result
        assert result["summarization_in_progress"] is False

    def test_route_to_next_agent_progression(self, game_state: GameState) -> None:
        """Test that routing progresses through turn queue correctly."""
        from langgraph.graph import END

        from graph import route_to_next_agent

        # After DM, should route to first PC
        game_state["current_turn"] = "dm"
        next_agent = route_to_next_agent(game_state)
        assert next_agent == "fighter"

        # After fighter, should route to rogue
        game_state["current_turn"] = "fighter"
        next_agent = route_to_next_agent(game_state)
        assert next_agent == "rogue"

        # After last PC (rogue), should signal END
        game_state["current_turn"] = "rogue"
        next_agent = route_to_next_agent(game_state)
        assert next_agent == END


# =============================================================================
# Error Handling Tests
# =============================================================================


@pytest.mark.integration
class TestErrorHandling:
    """Test error handling with LLM calls."""

    def test_invalid_model_name_error(
        self, mock_streamlit: MagicMock, pc_configs: dict[str, CharacterConfig]
    ) -> None:
        """Test that invalid model name produces appropriate error."""
        from agents import LLMError, dm_turn
        from models import (
            AgentMemory,
            DMConfig,
            GameConfig,
            GameState,
            NarrativeElementStore,
        )

        # Create state with invalid model
        bad_dm_config = DMConfig(
            name="Dungeon Master",
            provider="gemini",
            model="nonexistent-model-12345",
            token_limit=8000,
        )

        state = GameState(
            ground_truth_log=[],
            turn_queue=["dm"],
            current_turn="dm",
            agent_memories={"dm": AgentMemory(token_limit=8000)},
            game_config=GameConfig(),
            dm_config=bad_dm_config,
            characters=pc_configs,
            whisper_queue=[],
            human_active=False,
            controlled_character=None,
            session_number=1,
            session_id="test",
            summarization_in_progress=False,
            narrative_elements={},
            callback_database=NarrativeElementStore(),
        )

        with patch.dict("sys.modules", {"streamlit": mock_streamlit}):
            # Should raise LLMError with categorized error type
            with pytest.raises(LLMError):
                dm_turn(state)

    def test_error_categorization(self) -> None:
        """Test that errors are categorized correctly."""
        from agents import categorize_error

        # Test timeout error
        timeout_error = Exception("Connection timed out")
        assert categorize_error(timeout_error) == "timeout"

        # Test rate limit error
        rate_error = Exception("Rate limit exceeded, 429")
        assert categorize_error(rate_error) == "rate_limit"

        # Test auth error
        auth_error = Exception("Invalid API key")
        assert categorize_error(auth_error) == "auth_error"

        # Test network error
        network_error = Exception("Connection refused")
        assert categorize_error(network_error) == "network_error"


# =============================================================================
# Multi-Round Tests
# =============================================================================


@pytest.mark.integration
@pytest.mark.slow
class TestMultiRound:
    """Test multiple rounds of gameplay.

    These tests are marked slow as they make multiple LLM calls.
    """

    def test_two_consecutive_rounds(
        self, game_state: GameState, mock_streamlit: MagicMock
    ) -> None:
        """Test that two rounds can be executed consecutively."""
        from graph import run_single_round
        from models import GameState as GameStateType

        with (
            patch.dict("sys.modules", {"streamlit": mock_streamlit}),
            patch("persistence.save_checkpoint"),
            patch("persistence.get_latest_checkpoint", return_value=None),
            patch("graph._append_transcript_for_new_entries"),
        ):
            # First round
            result1 = run_single_round(game_state)
            if "error" in result1:
                pytest.skip(f"LLM error in round 1: {result1['error']}")

            log_length_after_round1 = len(result1["ground_truth_log"])

            # Second round using result of first
            state2: GameStateType = {
                "ground_truth_log": result1["ground_truth_log"],
                "turn_queue": result1["turn_queue"],
                "current_turn": "dm",  # Reset for new round
                "agent_memories": result1["agent_memories"],
                "game_config": result1["game_config"],
                "dm_config": result1["dm_config"],
                "characters": result1["characters"],
                "whisper_queue": result1["whisper_queue"],
                "human_active": result1["human_active"],
                "controlled_character": result1["controlled_character"],
                "session_number": result1["session_number"],
                "session_id": result1["session_id"],
                "summarization_in_progress": False,
            }

            result2 = run_single_round(state2)
            if "error" in result2:
                pytest.skip(f"LLM error in round 2: {result2['error']}")

            # Second round should have added more entries
            assert len(result2["ground_truth_log"]) > log_length_after_round1

    def test_context_accumulates_across_rounds(
        self, game_state: GameState, mock_streamlit: MagicMock
    ) -> None:
        """Test that context accumulates correctly across rounds."""
        from graph import run_single_round

        with (
            patch.dict("sys.modules", {"streamlit": mock_streamlit}),
            patch("persistence.save_checkpoint"),
            patch("persistence.get_latest_checkpoint", return_value=None),
            patch("graph._append_transcript_for_new_entries"),
        ):
            result = run_single_round(game_state)
            if "error" in result:
                pytest.skip(f"LLM error: {result['error']}")

            # Check that agent memories have content
            dm_memory = result["agent_memories"]["dm"]
            assert len(dm_memory.short_term_buffer) > 0

            # At least one PC should have memory entries
            pc_memories = [
                result["agent_memories"][name] for name in result["characters"].keys()
            ]
            has_pc_memory = any(len(m.short_term_buffer) > 0 for m in pc_memories)
            assert has_pc_memory, "At least one PC should have memory entries"


# =============================================================================
# Tool Binding Tests
# =============================================================================


@pytest.mark.integration
class TestToolBinding:
    """Test that tools are correctly bound to agents."""

    def test_dm_agent_has_dice_tool(self, dm_config: DMConfig) -> None:
        """Test that DM agent has dice rolling tool bound."""
        from agents import create_dm_agent

        agent = create_dm_agent(dm_config)

        assert agent is not None

    def test_pc_agent_has_dice_tool(
        self, pc_configs: dict[str, CharacterConfig]
    ) -> None:
        """Test that PC agents have dice rolling tool bound."""
        from agents import create_pc_agent

        for char_config in pc_configs.values():
            agent = create_pc_agent(char_config)
            assert agent is not None


# =============================================================================
# Response Quality Tests
# =============================================================================


@pytest.mark.integration
class TestResponseQuality:
    """Test the quality of LLM responses."""

    def test_dm_response_is_narrative(
        self, game_state: GameState, mock_streamlit: MagicMock
    ) -> None:
        """Test that DM response contains narrative content."""
        from agents import LLMError, dm_turn

        with patch.dict("sys.modules", {"streamlit": mock_streamlit}):
            try:
                result = dm_turn(game_state)
            except LLMError as e:
                if e.error_type == "rate_limit":
                    pytest.skip(f"Rate limit hit: {e}")
                raise

        dm_response = result["ground_truth_log"][0]

        # DM response should be non-trivial (more than a few words)
        content = dm_response.replace("[DM]:", "").strip()

        # Skip if model returned empty/minimal response (edge case)
        if content in ("", "[]", "None"):
            pytest.skip(f"Model returned empty/minimal response: {dm_response!r}")

        assert len(content) > 20, (
            f"DM response should be substantial, got: {dm_response!r}"
        )

    def test_pc_response_is_in_character(
        self, game_state_with_context: GameState, mock_streamlit: MagicMock
    ) -> None:
        """Test that PC response reflects character personality."""
        from agents import LLMError, pc_turn

        with patch.dict("sys.modules", {"streamlit": mock_streamlit}):
            try:
                result = pc_turn(game_state_with_context, "fighter")
            except LLMError as e:
                if e.error_type == "rate_limit":
                    pytest.skip(f"Rate limit hit: {e}")
                raise

        new_entries = result["ground_truth_log"][
            len(game_state_with_context["ground_truth_log"]) :
        ]
        assert len(new_entries) > 0

        # PC response should be non-trivial
        pc_response = new_entries[-1]
        content = (
            pc_response.split(":", 1)[-1].strip() if ":" in pc_response else pc_response
        )

        # Skip if model returned empty/minimal response (edge case)
        if content in ("", "[]", "None"):
            pytest.skip(f"Model returned empty/minimal response: {pc_response!r}")

        assert len(content) > 10, (
            f"PC response should be substantial, got: {pc_response!r}"
        )


# =============================================================================
# Configuration Validation
# =============================================================================


@pytest.mark.integration
class TestConfigValidation:
    """Test API key and config validation."""

    def test_validate_google_api_key(self) -> None:
        """Test that Google API key validation works."""
        from config import validate_google_api_key

        if not _GOOGLE_API_KEY:
            pytest.skip("No API key configured")

        result = validate_google_api_key(_GOOGLE_API_KEY)

        # Should be valid
        assert result.valid is True
        assert "valid" in result.message.lower() or len(result.message) > 0


# =============================================================================
# Summarizer Tests
# =============================================================================


@pytest.mark.integration
class TestSummarizer:
    """Test memory summarization with real LLM."""

    def test_summarizer_generates_summary(self, mock_streamlit: MagicMock) -> None:
        """Test that Summarizer can generate a summary."""
        from agents import LLMError
        from memory import Summarizer

        # Create summarizer with provider and model
        summarizer = Summarizer(provider="gemini", model=_PC_MODEL)

        # Test buffer to summarize
        buffer_entries = [
            "The party entered the dungeon.",
            "They fought goblins in the first room.",
            "The fighter was wounded but survived.",
            "They found a treasure chest with gold.",
        ]

        with patch.dict("sys.modules", {"streamlit": mock_streamlit}):
            try:
                summary = summarizer.generate_summary(
                    "dm",
                    buffer_entries,
                )
            except (LLMError, Exception) as e:
                error_str = str(e).lower()
                if "rate" in error_str or "429" in error_str or "quota" in error_str:
                    pytest.skip(f"Rate limit hit: {e}")
                raise

        # Summarizer returns empty string on error (graceful degradation)
        # Skip test if summary is empty - likely rate limited
        if summary == "":
            pytest.skip(
                "Summary empty - likely rate limited (summarizer handles errors silently)"
            )

        # Summary should be generated
        assert len(summary) > 0
        # Summary should be reasonable length (LLM may produce structured output
        # with headings/bullets that exceeds the raw input for short buffers)
        assert len(summary) < 2000  # Sanity cap â€” not unbounded
